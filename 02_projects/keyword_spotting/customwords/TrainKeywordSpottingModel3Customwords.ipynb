{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Deploy a Keyword Spotting Model on a Custom Dataset\n",
    "\n",
    "This Notebook ist based on HarvardX's [4-6-8-CustomDatasetKWSModel.ipynb](https://colab.research.google.com/github/tinyMLx/colabs/blob/master/4-6-8-CustomDatasetKWSModel.ipynb). This time we train our own custom keyword spotting model using your Custom Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf tensorflow log  v2.4.1.zip logs models train dataset extract_loudest_section\n",
    "apt-get update -qq && apt-get install -y --no-install-recommends wget unzip\n",
    "wget https://github.com/tensorflow/tensorflow/archive/v2.4.1.zip\n",
    "unzip v2.4.1.zip &> log\n",
    "mv tensorflow-2.4.1/ tensorflow/\n",
    "rm -rf v2.4.1.zip log\n",
    "python3 -m pip install --upgrade --quiet --no-cache-dir pip ffmpeg-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Packages**\n",
    "Import standard packages as well as the additional packages from the cloned Github Repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "# We add this path so we can import the speech processing modules.\n",
    "sys.path.append(\"./tensorflow/tensorflow/examples/speech_commands/\")\n",
    "import input_data\n",
    "import models\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create Custom Dataset**\n",
    "### Import the Dataset Basis\n",
    "We will build our dataset upon Pete Warden's dataset (a base set of \"other words\" and \"background noise\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Data Set Path for Python\n",
    "DATASET_DIR =  'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf speech_commands_v0.02.tar.gz*\n",
    "wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
    "mkdir dataset\n",
    "tar -xf speech_commands_v0.02.tar.gz -C 'dataset'\n",
    "rm -rf speech_commands_v0.02.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Import Recorded Audio Files\n",
    "Now upload all of your previously recorded custom audio files (aka the *.ogg files) by using my [Open Speech Recording Tool Start Script](https://github.com/KlausPuchner/TinyML/blob/main/02_projects/keyword_spotting/customwords/start-open-speech-recording-app.sh). Please select all/multiple files to upload them all at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FileUpload\n",
    "upload = FileUpload(multiple=True)\n",
    "upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, file_info in upload.value.items():\n",
    "    with open(name, 'wb') as fp:\n",
    "        fp.write(file_info['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert .ogg Files to .wav\n",
    "Now we convert them into correctly trimmed WAV files and then store them in the appropriate folders in the ```DATASET_DIR```.\n",
    "We will use Pete's extract_loudest_section tool which you can find more documentation about here: https://github.com/petewarden/extract_loudest_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "apt-get update -qqq && apt-get install -y --no-install-recommends -qqq git ffmpeg zip\n",
    "rm -rf wavs\n",
    "mkdir wavs\n",
    "find *.ogg -print0 | xargs -0 basename -s .ogg | xargs -I {} ffmpeg -i {}.ogg -ar 16000 wavs/{}.wav\n",
    "rm -rf *.ogg\n",
    "\n",
    "# then use pete's tool to only extract 1 second clips from them for use with the KWS pipeline\n",
    "mkdir trimmed_wavs\n",
    "git clone https://github.com/petewarden/extract_loudest_section.git\n",
    "make -C extract_loudest_section/\n",
    "/tmp/extract_loudest_section/gen/bin/extract_loudest_section 'wavs/*.wav' trimmed_wavs/\n",
    "rm -rf /wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store them in the appropriate folders\n",
    "data_index = {}\n",
    "os.chdir('trimmed_wavs')\n",
    "search_path = os.path.join('*.wav')\n",
    "for wav_path in glob.glob(search_path):\n",
    "    matches = re.search('([^/_]+)_([^/_]+)\\.wav', wav_path)\n",
    "    if not matches:\n",
    "        raise Exception('File name not in a recognized form:\"%s\"' % wav_path)\n",
    "    word = matches.group(1).lower()\n",
    "    instance = matches.group(2).lower()\n",
    "    if not word in data_index:\n",
    "      data_index[word] = {}\n",
    "    if instance in data_index[word]:\n",
    "        raise Exception('Audio instance already seen:\"%s\"' % wav_path)\n",
    "    data_index[word][instance] = wav_path\n",
    "\n",
    "output_dir = os.path.join('..', 'dataset')\n",
    "try:\n",
    "    os.mkdir(output_dir)\n",
    "except:\n",
    "    pass\n",
    "for word in data_index:\n",
    "  word_dir = os.path.join(output_dir, word)\n",
    "  try:\n",
    "      os.mkdir(word_dir)\n",
    "      print('Created dir: ' + word_dir)\n",
    "  except:\n",
    "      print('Storing in existing dir: ' + word_dir)\n",
    "  for instance in data_index[word]:\n",
    "    wav_path = data_index[word][instance]\n",
    "    output_path = os.path.join(word_dir, instance + '.wav')\n",
    "    shutil.copyfile(wav_path, output_path)\n",
    "os.chdir('..')\n",
    "!rm -r -f trimmed_wavs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Download your Dataset\n",
    "To zip and download your dataset run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r 3customworddataset.zip dataset\n",
    "from IPython.display import display, FileLink\n",
    "\n",
    "local_file = FileLink('./3customworddataset.zip', result_html_prefix=\"Click here to download: \")\n",
    "display(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf 3customworddataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train your Model**\n",
    "\n",
    "Next we need to select your keywords and model settings with which to train!\n",
    "\n",
    "WANTED_WORDS = A comma-delimited string of the words you want to train for (e.g., \"yes,no\").\n",
    "\n",
    "Since we collected the words \"activate\", \"deactivate\" and \"snapshot\" we make sure to input them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANTED_WORDS = \"activate,deactivate,snapshot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of training steps and learning rates can be specified as comma-separated strings to define the amount/rate at each stage. For example, TRAINING_STEPS=\"12000,3000\" and LEARNING_RATE=\"0.001,0.0001\" will run 12,000 training steps with a rate of 0.001 followed by 3,000 final steps with a learning rate of 0.0001. These are good default values to work off of when you choose your values as the course staff has gotten this to work well with those values in the past!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_STEPS = \"12000,3000\"\n",
    "LEARNING_RATE = \"0.001,0.0001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave the MODEL_ARCHITECTURE as tiny_conv the first time but if you would like to do this again and explore additional models some options are: single_fc, conv, low_latency_conv, low_latency_svdf, tiny_embedding_conv. **Do remember if you switch the model type you may need to update the C++ code to include the tflite::AllOpsResolver to make sure you have all of the neccessary ops!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of steps, which is used to identify the checkpoint\n",
    "# file name.\n",
    "TOTAL_STEPS = str(sum(map(lambda string: int(string), TRAINING_STEPS.split(\",\"))))\n",
    "\n",
    "# Print the configuration to confirm it\n",
    "print(\"Training these words: %s\" % WANTED_WORDS)\n",
    "print(\"Training steps in each stage: %s\" % TRAINING_STEPS)\n",
    "print(\"Learning rate in each stage: %s\" % LEARNING_RATE)\n",
    "print(\"Total number of training steps: %s\" % TOTAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of 'silence' and 'unknown' training samples required\n",
    "# to ensure that we have equal number of samples for each label.\n",
    "number_of_labels = WANTED_WORDS.count(',') + 1\n",
    "number_of_total_labels = number_of_labels + 2 # for 'silence' and 'unknown' label\n",
    "equal_percentage_of_training_samples = int(100.0/(number_of_total_labels))\n",
    "SILENT_PERCENTAGE = equal_percentage_of_training_samples\n",
    "UNKNOWN_PERCENTAGE = equal_percentage_of_training_samples\n",
    "\n",
    "# Constants used during training only\n",
    "VERBOSITY = 'DEBUG'\n",
    "EVAL_STEP_INTERVAL = '1000'\n",
    "SAVE_STEP_INTERVAL = '1000'\n",
    "\n",
    "# Constants for training directories and filepaths\n",
    "LOGS_DIR = 'logs/'\n",
    "TRAIN_DIR = 'train/' # for training checkpoints and other files.\n",
    "\n",
    "# Constants for inference directories and filepaths\n",
    "import os\n",
    "MODELS_DIR = 'models'\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODEL_TF = os.path.join(MODELS_DIR, 'KWS_custom.pb')\n",
    "MODEL_TFLITE = os.path.join(MODELS_DIR, 'KWS_custom.tflite')\n",
    "FLOAT_MODEL_TFLITE = os.path.join(MODELS_DIR, 'KWS_custom_float.tflite')\n",
    "MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'KWS_custom.cc')\n",
    "SAVED_MODEL = os.path.join(MODELS_DIR, 'KWS_custom_saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants which are shared during training and inference\n",
    "PREPROCESS = 'micro'\n",
    "WINDOW_STRIDE = 20\n",
    "\n",
    "# Constants for Quantization\n",
    "QUANT_INPUT_MIN = 0.0\n",
    "QUANT_INPUT_MAX = 26.0\n",
    "QUANT_INPUT_RANGE = QUANT_INPUT_MAX - QUANT_INPUT_MIN\n",
    "\n",
    "# Constants for audio process during Quantization and Evaluation\n",
    "SAMPLE_RATE = 16000\n",
    "CLIP_DURATION_MS = 1000\n",
    "WINDOW_SIZE_MS = 30.0\n",
    "FEATURE_BIN_COUNT = 40\n",
    "BACKGROUND_FREQUENCY = 0.8\n",
    "BACKGROUND_VOLUME_RANGE = 0.1\n",
    "TIME_SHIFT_MS = 100.0\n",
    "\n",
    "# Use the custom local dataset and set the tes/val/train split\n",
    "DATA_URL = ''\n",
    "VALIDATION_PERCENTAGE = 10\n",
    "TESTING_PERCENTAGE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "logs_base_dir='./logs/'\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir} --host 0.0.0.0 --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Launch Training**\n",
    "More information on the training script can be found  in the source code for the script [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/train.py). In short it sets up the optimizer and preprocessor based on all of the flags we pass in!\n",
    "\n",
    "Finally, by setting the VERBOSITY = 'DEBUG' above be aware that the training cell will print A LOT of information. Specifically you will get the accuracy and loss at each step as well as a confusion matrix every 1000 steps. We hope that is helpful in case TensorBoard fails to work. If you would like to run with less printouts you can change the setting to WARN or FATAL. You will find this in the \"Configure Your Model!\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tensorflow/tensorflow/examples/speech_commands/train.py \\\n",
    "  --data_dir={DATASET_DIR} \\\n",
    "  --data_url={DATA_URL} \\\n",
    "  --wanted_words={WANTED_WORDS} \\\n",
    "  --silence_percentage={SILENT_PERCENTAGE} \\\n",
    "  --unknown_percentage={UNKNOWN_PERCENTAGE} \\\n",
    "  --preprocess={PREPROCESS} \\\n",
    "  --window_stride={WINDOW_STRIDE} \\\n",
    "  --model_architecture={MODEL_ARCHITECTURE} \\\n",
    "  --how_many_training_steps={TRAINING_STEPS} \\\n",
    "  --learning_rate={LEARNING_RATE} \\\n",
    "  --train_dir={TRAIN_DIR} \\\n",
    "  --summaries_dir={LOGS_DIR} \\\n",
    "  --verbosity={VERBOSITY} \\\n",
    "  --eval_step_interval={EVAL_STEP_INTERVAL} \\\n",
    "  --save_step_interval={SAVE_STEP_INTERVAL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generating your Model**\n",
    "Just like with the pre-trained model we will now take the final checkpoint and convert it into a quantized TensorFlow Lite model.\n",
    "\n",
    "### **Generate a TensorFlow Model for Inference**\n",
    "Combine relevant training results (graph, weights, etc) into a single file for inference. This process is known as freezing a model and the resulting model is known as a frozen model/graph, as it cannot be further re-trained after this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {SAVED_MODEL}\n",
    "!python tensorflow/tensorflow/examples/speech_commands/freeze.py \\\n",
    "--wanted_words=$WANTED_WORDS \\\n",
    "--window_stride_ms=$WINDOW_STRIDE \\\n",
    "--preprocess=$PREPROCESS \\\n",
    "--model_architecture=$MODEL_ARCHITECTURE \\\n",
    "--start_checkpoint=$TRAIN_DIR$MODEL_ARCHITECTURE'.ckpt-'{TOTAL_STEPS} \\\n",
    "--save_format=saved_model \\\n",
    "--output_file={SAVED_MODEL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generate a TensorFlow Lite Model**\n",
    "Convert the frozen graph into a TensorFlow Lite model, which is fully quantized for use with embedded devices.\n",
    "\n",
    "The following cell will also print the model size, which will be under 20 kilobytes.\n",
    "\n",
    "We download the dataset to use as a representative dataset for more thoughtful post training quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings = models.prepare_model_settings(\n",
    "    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),\n",
    "    SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
    "    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
    "audio_processor = input_data.AudioProcessor(\n",
    "    DATA_URL, DATASET_DIR,\n",
    "    SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,\n",
    "    WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,\n",
    "    TESTING_PERCENTAGE, model_settings, LOGS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if the below cell fails it might be because you do not have enough data to have 100 recordings in the representative dataset! If this happens you will see an error that says something like ValueError: cannot reshape array of size 0 into shape (1,1960). To help you fix this we have added a print(i) into the loop. As such, all you have to do is change the REP_DATA_SIZE variable to be equal to the last integer value printed out by the loop and then re-run the cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REP_DATA_SIZE = 100\n",
    "with tf.Session() as sess:\n",
    "  float_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
    "  float_tflite_model = float_converter.convert()\n",
    "  float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
    "  print(\"Float model is %d bytes\" % float_tflite_model_size)\n",
    "\n",
    "  converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  converter.inference_input_type = tf.lite.constants.INT8\n",
    "  converter.inference_output_type = tf.lite.constants.INT8\n",
    "  def representative_dataset_gen():\n",
    "    for i in range(REP_DATA_SIZE):\n",
    "      data, _ = audio_processor.get_data(1, i*1, model_settings,\n",
    "                                         BACKGROUND_FREQUENCY, \n",
    "                                         BACKGROUND_VOLUME_RANGE,\n",
    "                                         TIME_SHIFT_MS,\n",
    "                                         'testing',\n",
    "                                         sess)\n",
    "      flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(1, 1960)\n",
    "      print(i)\n",
    "      yield [flattened_data]\n",
    "  converter.representative_dataset = representative_dataset_gen\n",
    "  tflite_model = converter.convert()\n",
    "  tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
    "  print(\"Quantized model is %d bytes\" % tflite_model_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the accuracy after Quantization\n",
    "\n",
    "Verify that the model we've exported is still accurate, using the TF Lite Python API and our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference\n",
    "def run_tflite_inference_testSet(tflite_model_path, model_type=\"Float\"):\n",
    "  #\n",
    "  # Load test data\n",
    "  #\n",
    "  np.random.seed(0) # set random seed for reproducible test results.\n",
    "  with tf.Session() as sess:\n",
    "    test_data, test_labels = audio_processor.get_data(\n",
    "        -1, 0, model_settings, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n",
    "        TIME_SHIFT_MS, 'testing', sess)\n",
    "  test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n",
    "\n",
    "  #\n",
    "  # Initialize the interpreter\n",
    "  #\n",
    "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
    "  interpreter.allocate_tensors()\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "  \n",
    "  #\n",
    "  # For quantized models, manually quantize the input data from float to integer\n",
    "  #\n",
    "  if model_type == \"Quantized\":\n",
    "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "    test_data = test_data / input_scale + input_zero_point\n",
    "    test_data = test_data.astype(input_details[\"dtype\"])\n",
    "\n",
    "  #\n",
    "  # Evaluate the predictions\n",
    "  #\n",
    "  correct_predictions = 0\n",
    "  for i in range(len(test_data)):\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    top_prediction = output.argmax()\n",
    "    correct_predictions += (top_prediction == test_labels[i])\n",
    "\n",
    "  print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
    "      model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute float model accuracy\n",
    "run_tflite_inference_testSet(FLOAT_MODEL_TFLITE)\n",
    "\n",
    "# Compute quantized model accuracy\n",
    "run_tflite_inference_testSet(MODEL_TFLITE, model_type='Quantized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a TensorFlow Lite for Microcontrollers Model\n",
    "To convert the TensorFlow Lite quantized model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers on Arduino we simply need to use the xxd tool to convert the .tflite file into a .cc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update -qqq && apt-get -qqq install xxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TFLITE = './models/model.tflite'\n",
    "MODEL_TFLITE_MICRO = './models/model.cc'\n",
    "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
    "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
    "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated Tensorflow Lite for Microcontroller model can now be used in the Arduino IDE. There are two options to do this:\n",
    "\n",
    "1. Copy the screen output directly from the Jupyter Notebook into the **micro_features_model.cpp** file (in the Arduino IDE)\n",
    "2. Download the **model.cc** file for later use to copy its content into the **micro_features_model.cpp** file (in the Arduino IDE)\n",
    "\n",
    "### Option 1: Copy Output directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {MODEL_TFLITE_MICRO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Option 2: Download Model File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "local_file = FileLink('./models/model.cc', result_html_prefix=\"Click here to download: \")\n",
    "display(local_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
